{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aefbf6e",
   "metadata": {},
   "source": [
    "### Local Model Definition\n",
    "The create_local_model() function defines the architecture of the lightweight neural network used for both local training and the global model. The model consists of two hidden dense layers with ReLU activation functions and dropout for regularization, followed by a sigmoid output layer suitable for binary classification. This standardized structure ensures consistency across all client models, which is crucial for weight aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57674976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Model Definition\n",
    "def create_local_model(input_dim=2):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5c8ea",
   "metadata": {},
   "source": [
    "### 2. Local Model Training\n",
    "During each training round, a new local model is instantiated per device and initialized with the current global model weights. The model is then trained on locally available data using a differentially private optimizer (DPKerasSGDOptimizer) to ensure that each update preserves user privacy. Training is conducted for one epoch per device to simulate a real-world on-device federated learning scenario. After training, local weights, loss, and accuracy are recorded for each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Round\n",
    "for i, client_dataset in enumerate(client_data):\n",
    "    local_model = create_local_model()\n",
    "    local_model.set_weights(global_model.get_weights())\n",
    "    local_optimizer = tfp.DPKerasSGDOptimizer(\n",
    "        l2_norm_clip=1.0,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        num_microbatches=1,\n",
    "        learning_rate=0.01\n",
    "    )\n",
    "    local_model.compile(\n",
    "        optimizer=local_optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    logging.info(f\"Training local model on device {i + 1}\")\n",
    "    history = local_model.fit(client_dataset, epochs=1, verbose=0)\n",
    "    \n",
    "    local_weights.append(local_model.get_weights())\n",
    "    local_losses.append(history.history['loss'][0])\n",
    "    local_accs.append(history.history['binary_accuracy'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd66f08",
   "metadata": {},
   "source": [
    "### 3. Model Aggregation (Federated Averaging)\n",
    "Once all local models are trained, their weights are aggregated using the Federated Averaging (FedAvg) algorithm. This method computes the element-wise mean of the model weights from each device to form an updated global model. This central model is then redistributed to clients in the next round. FedAvg is a key step in federated learning, allowing global updates without centralized data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ac3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_weights(local_weights):\n",
    "    return [np.mean([w[i] for w in local_weights], axis=0) for i in range(len(local_weights[0]))]\n",
    "\n",
    "global_weights = aggregate_weights(local_weights)\n",
    "global_model.set_weights(global_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416ecb8",
   "metadata": {},
   "source": [
    "### 4. Central Model Initialization and Synchronization\n",
    "At the beginning of training, a central global model is initialized with the same architecture as the local models. It acts as the synchronized model shared across all clients and is continuously updated after each round of aggregation. The synchronization process ensures that every client trains from the same starting point in every round, maintaining consistency across decentralized learning processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ad1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = create_local_model()\n",
    "optimizer = tfp.DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=1.0,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=1,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "global_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af525bf6",
   "metadata": {},
   "source": [
    "### 5. Privacy and Performance Metrics\n",
    "After each round, key performance and privacy metrics are computed. Mutual information is used to estimate potential privacy leakage from model updates. The dp_accounting library computes the differential privacy budget (ε) using Rényi Differential Privacy accounting. Additional metrics such as latency, global accuracy, and loss are tracked to monitor model convergence and privacy trade-offs across training rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5936e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak = compute_mutual_information(raw_data[0][0], updates)\n",
    "lat = np.random.uniform(0.1, 0.5) * num_devices\n",
    "\n",
    "accountant = dp_accounting.rdp.RdpAccountant()\n",
    "steps = (total_examples // batch_size) * (round_num + 1)\n",
    "event = dp_accounting.GaussianDpEvent(noise_multiplier=noise_multiplier)\n",
    "accountant.compose(dp_accounting.SelfComposedDpEvent(event, steps))\n",
    "epsilon = accountant.get_epsilon(target_delta=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab10dc",
   "metadata": {},
   "source": [
    "###  6. Metrics Logging and Model Saving\n",
    "All performance metrics, including accuracy, loss, leakage, latency, and privacy budget (ε), are saved for analysis. The final global model weights are saved locally for reproducibility or deployment. This step ensures traceability and provides insights into the model’s privacy-utility trade-offs over multiple federated learning rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c36f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('ftl_app/precomputed/weights', exist_ok=True)\n",
    "global_model.save_weights('ftl_app/precomputed/weights/global_model.h5')\n",
    "\n",
    "with open('ftl_app/precomputed/metrics.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'accuracy': accuracy, 'loss': loss,\n",
    "        'leakage': leakage, 'latency': latency, 'epsilon': epsilon_values\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d57b1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "962f57d2",
   "metadata": {},
   "source": [
    "### Full MOdel Pipelein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftl_app.ftl_core.ftl_models import run_ftl_simulation\n",
    "import json\n",
    "\n",
    "accuracy, loss, leakage, latency, epsilon = run_ftl_simulation(num_rounds=10, num_devices=10, noise_multiplier=1.1)\n",
    "with open('ftl_app/precomputed/metrics.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'accuracy': accuracy, 'loss': loss, 'leakage': leakage, \n",
    "        'latency': latency, 'epsilon': epsilon\n",
    "    }, f)\n",
    "print(\"Precomputation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "red_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
